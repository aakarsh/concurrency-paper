% Created 2014-04-04 Fri 19:09
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{soul}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{listings}

\tolerance=1000
\providecommand{\alert}[1]{\textbf{#1}}

\title{Synchronization in Linux}
\author{Aakarsh Nair}
\date{\today}

\begin{document}


\lstdefinelanguage{anX86} {
	morekeywords={
		nop,movl,pushl,popl,cmpl,testl,leal,cmovl,jmp,je,jne,
		jz,jnz,jg,jge,jl,jle,addl,subl,imul,idiv,cdq,incl,decl,
		negl,andl,orl,xorl,notl,shrl,shll,sarl,sall,ret,leave,
		call,setg,setl,setge,setle,setne,sete,movzbl,
		eax,ebx,ecx,edx,edi,esi,ebp,esp,al,bl,cl,dl,
                pause,lock,decb,jns,jle,jmp,cmpb,movb,xchg,xchgb
	},
	sensitive=true,
	morecomment=[l]{\#},
	morestring=[b]"
}

\lstset{language=C}  
\maketitle

\setcounter{tocdepth}{3}
\tableofcontents

\maketitle
\vspace*{1cm}

\section{Introduction}

Synchronization becomes necessary when the outcome of a computation
depends on how two or more interleaved kernel control paths are
nested. Critical regions are parts of code that must be executed by at
most one kernel control path to completion before another kernel
control path is allowed to execute it.


\section{Kernel Preemption}

Process running in Kernel Mode can be replaced by another process.
The main reason for making a kernel preemptive is to reduce the
dispatch latency of user mode processes (the time between when they
become runnable and begin running).  Process switches happens via
the\lstinline{switch_to_macro}.

Kernel premption is enabled and disabled via the
\lstinline{prempt_count} in the \lstinline{thread_info}. A tast is
premptible if \lstinline{thread_info()->preempt_count} is zero.

The \lstinline{prempt_count} is greater than zero if 

\begin{itemize}
\item The kernel is executing in an interrupt service routine.
\item kernel is executing a tasklet of a softirq
\item kernel preemption has been explicitly disabled setting it to a
  positive value
\end{itemize}


The \lstinline{prempt_count} is an amalgamation of three separate
counters meant to keep track number of times kernel preemption count
as well as softirq and hardirq counts.


\begin{center}
  \begin{tabular}{ l | l }    
    \hline
    Bits & Description                    \\ \hline
    0-7 & Preemption counter (max=255)    \\ 
    8-15 & Softirq counter (max=255)      \\ 
    16-27 &  Hardirq counter (max=4096)   \\ 
    28 & \lstinline{PREEMPT_ACTIVE} flag  \\ \hline
  \end{tabular}
\end{center}

Thus kerenel is preempted only when its executing an exception handler
and kerenel preemption has not been explicitly disabled and the local
CPU has local interrupts enabled.

Key Macors Desling with preemption counter are given as 

\begin{center}
  \begin{tabular}{ l | p{9 cm} }    
    \hline
    Bits & Description                    \\ \hline
    \lstinline{preempt_count()} & Select the \lstinline{preempt count} from \lstinline{thread_info}    \\ 
    \lstinline{preempt_disable()} & Increase the value of the preemption counter.     \\ 
    \lstinline{preempt_enable_no_resched()} &  Decrease by one the value of the preemption counter  \\
    \lstinline{preempt_enable()} & Decrease by one the value of preemption counter and call \lstinline{preempt_schedule()} if \lstinline{TIF_NEED_RESCHED} on \lstinline{thread_info} is set \\
    \lstinline{get_cpu()} & Similar to \lstinline{preempt_disable()} but also returns the number of local CPU \\
    \lstinline{put_cpu()} & Same as  \lstinline{preempt_enable()} \\
    \lstinline{put_cpu_no_resched()} & Same as \lstinline{preempt_enable_no_resched()}    \\
    \hline
  \end{tabular}
\end{center}


\section{Kernel Synchronization Primitives}

The linux kernel provides several synchronization primitives these
include

\begin{itemize}
\item \textbf{Per-CPU variables}
  Use duplicate data structures for each cpu
\item \textbf{Atomic Operations}
  Atomically read-modify-write instruction to a counter
\item \textbf{Memory Barrier}
  Avoid instruction reordering
\item  \textbf{Spinlock}
  Lock with a busy wait
\item \textbf{Semaphores}
  Lock with blocking wait/sleep.
\item \textbf{Seqlocks}
  Lock based on access counter
\item \textbf{Local interrupt disabling}
  Forbid interrupt handling on a single CPU
\item \textbf{Local softirq disabling}
  Forbid deferrable function handling on a single CPU
\item \textbf{Read Copy Update}
  Lock free access to shared data structures using pointers
\end{itemize}

Many of these synchronization constructs depend on implementation of
atomic operations implemented at the chip level on CPUs. Thus are
specific to the architecture they are executing on.

\subsection{Processor guarantees}

\begin{itemize}
  \item On a particular CPU dependent memory accesses happen in order
    of issuance.

  \item Overlapping loads and stores within a particular CPU will
    appear to be ordered within a CPU
    
\end{itemize}

Things which cannot be assumed

\begin{itemize}
\item it \textbf{must not} be assumed that compilers will not reorder
  memory references not protected with ACCESS\_ONCE(). Without
  ACCESS\_ONCE() the compiler can perform transformations see memory
  barriers.
\item it \textbf{must not} be assumed that independent loads and
  stored will be issued in any given order.
\item it \textbf{must not} be assumed that overlapping memroy accesses
  may be merged or discarded.  
\end{itemize}

Things that we anti-guarantees (bad stuff will happen):

\begin{itemize}


\item Compilers will often generate code to modify bit fields using non
  atomic read-modify write sequences


\item All fields in a given bit-field must be must be protected by one
  lock. Update of one field can be corrupted by another by compiler.
  
\item Guarantees only apply to properly aligned and sized scalar
  variables. Same size as "char","short","int" and "long".  
\end{itemize}



\subsection{Memory Barrier}

Memory barriers impose perceived partial ordering over memory
operations on either side of the barrier.

Memory barriers provide a way to instruct the compiler and CPU to
restrict the order in which instructions are executed. Performance
optimizations of compilers and CPUs play havoc with synchronization
primitives. These include compiler reordering instructions to optimize
register usage, CPUs executing instructions in parallel, reordering
memory accesses.


Types of performance tricks memory barriers may protect against are
\begin{itemize}
\item reordering and deferral of combination of memory operations
\item speculative loads
\item speculative branch prediction and caching
\end{itemize}

Memory barrier types include
\begin{itemize}
\item Write (or store) memory barrier

  A write memory barrier gives gauratnee that all STORE operations
  specified before the barrier will happen before all STORE operations
  STORE oeprations specified after the barrier. wrt other components
  in the system.

  A partial order on STORES only. Does not affect LOADS.

  CPU can be viewed as performing a commit. All stores before the
  write barrier will occur before all stores after the write barrier.

  Should not be paired with read or data dependency barrier.
  
\item Data dependency barrier

  Weaker form of read barrier. Two loads such that the the second
  depnds on result of first ( first load retreives the address which
  to which second load is directed). Ensure the target of second load
  is updated before first load accessed (??)

  Partial ordering on inter-dependent loads only. No effect on
  independent loads or overlapping loads.


  TODO - need to look into this deeper.


\item Read (or load) memory barrier  

  
  
\item General Memory barrier

  All LOAD and STORE operations before the barrier will happen before
  all LOAD and STORE operations specified after the barrier with
  respect to other components of the system.

  A partial ordering over both loads and stores.  

\item Implicet Barriers
  \begin{itemize}
  \item ACQUIRE operations
  \item RELEASE operations
  \end{itemize}
\end{itemize}



\subsection{CPU Memory Barriers}

All memory barriers except datadependecy barriers imply compiler
barrier. SMP memory barriers are reduced to compier barriers on
uniprocessor compiled systems. 


\begin{itemize}

\item{Optimization barrier} Can be implemented using the
  \emph{barrier()} macro expanding to \emph{asm volatile"":::memory}

  Tells the compiler to insert empty assembly fragment. While the
  volatile keyword forbids the compiler from shuffling the
  instruction.  The memory keyword forces the compiler to use memory
  locations instead of those stored in the register. The CPU can still
  mix assembly instruction


  
  
\item{Memory barrier}

  In 80x86 list of serializing instructions which act as memory
  barriers:


  \begin{itemize}
    \item I/O port operations
    \item instructions with lock byte
    \item instructions affecting the IF flag in eflags register such as those instructions which write to registers
      \begin{itemize}
        \item control registers (cli)
        \item system  registers (sti)
        \item debug   registers          
      \end {itemize}
    \item Some instructions introduced in Pentium 4
      \begin{itemize}
        \item lfence - read barriers
        \item sfence - write barries
        \item mfence - read write barriers
      \end{itemize}
    \item Speial instructions - iret terminating interrupt or exception handler                  
  \end{itemize}
  
\end{itemize}

Read barriers maintain the serial order of read instructions, write
barriers maitain serial order of write instructions.

\begin{center}
  \begin{tabular}{ l | l }    
    \hline
    Function/Macors & Description \\ \hline
    mb() & Memory barrier for MP and UP \\ 
    rmb() & Read memory barrier for MP and UP  \\ 
    wmb() &  Write memory barrier for MP and UP \\
    smp\_mb() &  Memory barrier for MP only \\
    smp\_rmb() &  Read memory barrier for MP only \\
    smp\_wmb() &  Write memory barrier for MP only \\
    \hline
  \end{tabular}
\end{center}
  Macro expansions on  80x86


  \begin{tabular}{ l | p{5 cm} }    
    \hline
    Function/Macors & Description \\ \hline
    mb() &  \\ 
    rmb() & asm volatile ("lfence") or asm volatile ("lock;addl 0,0(\%\%esp)":::"memory")  \\ 
    wmb() & barrier()- intel never reorders write memory access only need an optimization barrier  \\
    smp\_mb() &   \\
    smp\_rmb() &  \\
    smp\_wmb() &  \\
    \hline
  \end{tabular}


All atomic operations act as memory barriers since they use the lock byte.

\subsection{Atomic Operations}

Atomic read-modify-write instructions are executed atomically by a CPU
hardware via a single instruction which is executed without
interruptions by other CPUs.

\subsubsection{Atomic Operations 80x86}

\begin{itemize}
  \item Read-modify-write assembly instructions such as inc and dec
    that read data from memory and update it are atomic , provided
    stale data has not been read by another processor. This is the
    case in uniprocessor systems.

  \item Read-modify-write instructions whose opcode is prefixed by the
    \emph{lock byte} (\emph{0xf0}) are atomic on multiprocessor
    systems.  The lock byte will lock access to the mememory bus until
    the locking instruction finishes its operation.
  \item Instructions prefixed by the \emph{rep} byte \emph{0xf2,0xf3}
    which forces instructions to be repeated are not atomic since the
    CPU checks interrupts before each iteration.

\end{itemize}



\subsection{Spinlock}

Spinlock is implemented by the \lstinline{spinlock_t} which consists
of two fields:

\begin{itemize}
  \item \textbf{slock} Encodes the spinlock state. Value 1 corresponds
    to unlocked state, Negative values and 0 denote locked state.
  
  \item \textbf{break\_lock} Flag signaling that a process is busy
    waiting for the lock. Present on SMP systems with kernel
    preemption.    
\end{itemize}


Some functions that initialize, acquire and relsease spin locks are
given in the table.

  \begin{center}
  \begin{tabular}{ |l | p{7cm}| }    
    \hline
    Function/Macors & Description \\ \hline
    spin\_lock\_init() & Set spin lock to 1 (unlocked) \\
    \hline
    spin\_lock() &
    Cycle until spinlock becomes 1(unlocked) then set it to 0 (locked) \\
    \hline
    spin\_unlock() & Set the spin lock to 1 (unlocked) \\
    \hline
    spin\_unlock\_wait() & Wait until the spinlock becomes 1 (unlocked)  \\
    \hline
    spin\_is\_locked() & Return 0 if the spinlock is set to 1(unlocked) ; 1 otherwise  \\
    \hline
    spin\_trylock() &  Set the spin lock to 0 (locked), and return 1 if the previous value of the lock was 1; 0 otherwise\\
    \hline
  \end{tabular}
  \end{center}

  \subsubsection{spin\_lock with kernel preemption}

  \begin{itemize}
  \item Invoke \lstinline{preeempt_disable()} disable kernel preemption
    
  \item Invoke \lstinline{_raw_spin_trylock()} atomic test and set on
    spinlock's \lstinline{slock} field.

      \begin{lstlisting}[language=anX86]
      movb $0,%a1
      xchgb %a1,slp->slock
      \end{lstlisting}


      \lstinline{xchg} exchange atomically content of 8-bit
      \lstinline{%a1} with \lstinline{slp->slock}.  return 1 if old
      value was positive or 0 otherwise.

    \item If old value of spin lock was positive, we have acquired the
      spinlock.
        
    \item If failed then spin lock was not positive , invoke
      \emph{preempt\_enable()} decrements the preeempt counter. which
      if it goes to zero will allow the process to be scheduled out,

    \item set the \emph{break\_lock} field to one. Allow another
      process to release spin lock prematurely

    \item Execute the wait cycle
      \begin{lstlisting}
      while(spin_is_locked(slp) && slp->break_lock)
          cpu_relax(); // special pause instruction Pentium 4
      \end{lstlisting}
    \item Jump back to step 1       
  \end{itemize}


  \subsubsection{spin\_lock (no kernel preemption)}

  The following tight busy wait is implemented. An atomic decrement of
  the spinlock is performed using the \lstinline{lock} prefix on the
  decrement operation. A test is performed on sign flag if it is
  positive we continue with instruction 3. Otherwise tight loop at 2
  is executed until the spinlock is positive. When spinlock attains
  positive value the execution will restart at label 1 where we will
  try to atomically decrement the spin lock.


\begin{lstlisting}[language=anX86]
  1:  lock; decb slb->slock
      jns 3f
  2:  pause
      cmpb $0,slp->slock
      jle 2b
      jmp 1b
  3: 
\end{lstlisting}

\subsubsection{spin\_unlock}

This releases an acquired spin lock. Executes the assembly instruction:


\begin{lstlisting}[language=anX86]
  movb $1,slp->slock
\end{lstlisting}

Then invoking \lstinline{preempt_enable()}. on x86 the lock byte is
not used since since write-only access in memory are atomically
executed.

\subsubsection{Read/Write Spin Locks}

Allows serveral simultaneous reads of the same data structure as long
as no kernel control path modifies it. Writer control paths acquire
write locks granting exclusive access. Ability to perform concurrent
reads improves system performance.

Read/Write spinlocks are represented by \lstinline{rwlock_t} structure
with a 32-bit lock field

The bit field \lstinline{lock} is devided into a counter and a flag
\begin{itemize}
\item \textbf{counter} 24 bit field , Number of kernel control paths
  in reading in the critical section.
  \item \textbf{flag} An unlocked flag at bit 24. Set when there are no
    control paths in critical section. othersize cleared.
\end{itemize}

Thus lock is \emph{0x01000000} when spin lock idle and
\emph{0x00000000} when writing. and any number after \emph{0x00ffffff}
when one thread is reading , \emph{0x00fffffe} when on thread is
reading, i.e stored in twos complement format.

Like ordinary spinlock we also include a \lstinline{break_lock} field.

\subsection{Reading with read/write spinlock}

When acquiring the read lock the \lstinline{read_lock} macro will use
the \lstinline{_raw_read_trylock()} function.

\begin{lstlisting}
  int _raw_read_trylock(rwlock_t *lock)
  {
    atomic_t *count = (atomic_t *)lock->lock;
    atomic_dec(count);
    if (atomic_read(count) >= 0)
       return 1;
    atomic_inc(count);
    return 0
   }
\end{lstlisting}



\subsection{Semaphores}



\subsection{Per-CPU Variables}

A per-CPU variable is an array of data structures one per cpu. Each CPU
can read and modify its own elements without race.

These are aligned in main memory so that each one falls in different
line of hardware cache. Concurrent access does not lead to cache line
snooping and invalidation.

Though per-cpu variables allow for data structures to be protected
against different CPUS they cannot be used to protect against
asynchronous accesses coming through the same CPU.

Accessing a per-CPU variable needs to be aware that premption might
cause the process to get migrated onto another CPU and thus it might
be advisable to turn off preemptions for per CPU variables.


\begin{center}
  \begin{tabular}{ l | l }
    
    \hline
    Function/Macors & Descriptions \\ \hline
    DEFINE\_PER\_CPU(type,name) & Statically Allocate a per-CPU called type \\ 
    per\_cpu(name,cpu) & Selects element for CPU cpu \\ 
    get\_cpu\_var(name) &  Disables kernel premeption \\
    put\_cpu\_var(name) &  Re-enables kernel premeption \\
    alloc\_percpu(ptr) &  dynamcally allocatee cpu variable \\
    free\_percpu(ptr) &  Release a dynamcally allocated cpu variable \\
    \hline
  \end{tabular}
\end{center}

\section{MESI Cache Coherency Protocol}

Cache-coherency protocls manage cache-line states to prevent
inconsitent states or lost data. MESI stats for "modified",
"exclusive", "shared" and "invalid". Which represent the four states
assigned to cache lines in this protocol.The MESI protocol is a
protocol used to implement cache and memory coherency amongst multiple
CPUs. \cite{Birdetal2001}

Two bits are added to each cache line which represent the four states
that a cache line can be in.

\begin{itemize}
\item Modified
  \begin{itemize}
    \item Cache Line is present only on current CPU
    \item Cache Line has been modified
    \item Write back needs to be performed
  \end{itemize}


\item Exclusive
    \begin{itemize}
    \item Cache Line is present only in current CPU
    \item Cache Line is cliean (matches main memory)
    \end{itemize}
    
  \item Shared
    \begin{itemize}
    \item Cache Line is present in multiple CPUs
    \item Cache Line is Clean (matches main memory)
    \end{itemize}
  \item Invalid 
\end{itemize}

\subsection{MESI Protocol messages}

If the CPUs are on a single shared bus we only require the following
messages on the bus.

\begin{itemize}
\item \textbf{Read} Content : physical address of cache line to be
  read.
\item \textbf{Read Response} Response with data, Source : Memory or
  one of the other caches. If the other cache has data in modified
  state.
\item \textbf{Invalidate} Content : Address of cache line to be
  invalidated. All other caches must invalidate cache line with this
  address.

\item \textbf{Invalidate Acknowledge} From: CPU that has invalidated a
  cache line.

\item \textbf{Read Invalidate} Content: Read cache line and take
  ownership of it getting the cache line removed from other
  processors. Combination of read and invalidate. Responses are read
  response and invalidate acknowledge

\item \textbf{Writeback} contnet: Address of and data of write back to
  memory. This message might get snooped by other processors to mark
  cache lines as "modified"  
\end{itemize}

Thus we see the fractal nature of distributed system where message
passing is implemented at different levels of the systems
architecture.


\subsection{MESI Transitions Table}

We give a tabular description of transitions involved in the MESI
protocl.



\begin{tabular*}{0.75\textwidth} {| l | l | l | p{5cm}| }    
    \hline
    \# & Start  & End   & Descriptions \\
    \hline
    a & Modified  & Exclusive &
    Cache line is written back to memmory but CPU retains exclusive 
    owner ship of and right to modify it. Requires "writeback" 
    message\\
    \hline
    b& Exclusive & Modified &  
    CPU writes to exclusive cache line. No messages required. \\
    \hline
    c & Modified & Invalid &   
    CPU receives a "read invalidate" for a cache line it modified.It 
    must invalidate the local copy, send an read response and a 
    invalidate acknowledge \\
    \hline
    d & Invalid & Modified &
    An atomic read-modify-write operation on a data item not present
    in the cache. Transmits a "read invalidate" response and gets
    "read response". Cannot proceed until all cpus reply with
    "invalidate acknowledge" response.    \\

    \hline
    e & Shared & Modified & 

    CPU does read-modify write one data item that was
    read-only. Transmits a "invalidate" messagee. Wait for invalidate
    acknowlege from all other CPUs.    
    \\
    \hline
    f & Modified & Shared &

    Some other CPU reads from our cache line suplied from this
    CPU. This CPU responds with a "read response" message.    
    \\    
    \hline

    g & Exclusive & Shared & 


    Some other CPU reads data in this cache line supplied from this
    CPU or from memory. If this CPU retains a read-only copy. This cpu
    will respond with a read response with requested data.
    
    \\
    \hline
    h & Shared & Exclusive & 

    This CPU is about to write to shared item. Transmits an invalidate
    message to other CPUs Waits for full "invlidate acknowledge"
    repsonse. Or all other CPUs had to drop the cache line making this
    CPU the sole owner. \\
    \hline
    i & Exclusive & Invalid & 

    Another CPU ran an atomic read-modify-write in cache line held by
    current CPU. Received a "read invalididate". This CPU will respond
    with a "read response" and "invalidate acknowledge" message.   \\

    \hline

    j & Invalid & Exclusive &

    CPU does a store to data not in cache. Transmits a "read
    invalidate" message. The CPU must wait for all other processors to
    "invalidate acknowledge". An expected next state will be
    "modified"  
    \\
    \hline

    k & Invalid & Shared &
    
    The CPU loads item not in cache and completes a transition on read
    a "read response"     \\


    \hline
    l & Shared & Invalid &
    
    Some ohter cpu does a store to data item. On the reception of a
    "invalidate" message. Responds with a "invalidate acknowledge"
    messaage. \\

    \hline    
  \end{tabular*}


Store performance for a first write is poor.


\subsubsection{Store Buffers}
Added between CPU and cache
\subsubsection{Store Forewarding}
\subsubsection{Store Buffers and Memory Barriers}


\subsection{Summary \& Acknowledgement}

Lot of the text is adaptation of Chapter 5 of Linux Kernel Programming
by Bovett and the mememory barrier documentation part of the linux
kernel David Howells and Paul E. McKenney. Any errors are entirely my
own. For people looking for definitive and trustworthy accounts
looking at these sources is recomended.

\begin{thebibliography}{9}

\bibitem{ia32-sys}
  \textit{IA-32 Intel Architecture Software Developer's Manual, Volume 3:
    System Programming Guide} \\
  \textit{Chapter 7.1: Locked Atomic Operations} \\
  \textit{Chapter 7.2: Memory Ordering}  \\
  \textit{Chapter 7.4: Serializing Instructions}

\bibitem{unix-ma}
  \textit{Unix Systems for Modern Architectures, Symmetric Multiprocessing and Caching
    for Kernel Programmers} \\
  \textit{Chapter 13: Other Memory Models}  
  
\bibitem{mesi-wikipedia} 
  \textit{MESI protocol}  \\
  \url{http://en.wikipedia.org/wiki/MESI_protocol}
\bibitem{cache-coherency-primer} 
  \textit{Cache Coherency Primer}  \textit{Fabian "ryg" Giesen} \\
  \url{https://fgiesen.wordpress.com/2014/07/07/cache-coherency/}
 \bibitem{atomic-operations}
   \textit{Atomic Operations - CSE 378 University of Washington} \\
  \url{http://courses.cs.washington.edu/courses/cse378/07au/lectures/L25-Atomic-Operations.pdf}
\bibitem{memory-barriers}
  \textit{Linux Kernel Memory Barriers} \\
  \url{https://www.kernel.org/doc/Documentation/memory-barriers.txt}

\bibitem{columbia-sync-linux}
  \textit{Synchronization in Linux} \\
  \url{http://www.cs.columbia.edu/~junfeng/10sp-w4118/lectures/l11-synch-linux.pdf}

\bibitem{under-linux-kernel-sync}
  \textit{Understanding the Linux Kernel - Chapter 5} \\
  \textit{David Bovet and Marco Cesati}  

\bibitem{sadc}  
  \textit{Structures and Design of Computers } \\
  \textit{David E. Patterson and J.L. Hennessey} 
  
\bibitem{whymem}
  \textit{Why memory barriers?} \\
  \textit{Paul McKay} \\
  \url{http://www.rdrop.com/users/paulmck/scalability/paper/whymb.2010.06.07c.pdf}
\end{thebibliography}

\end{document}
